{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0597daa",
   "metadata": {},
   "source": [
    "##  ðŸ“˜ AI Handbook Assistant (RAG-Based Project)\n",
    "**Project:** Generative AI & LLMs  \n",
    "**Student:** SHUBHAM KUMAR\n",
    "\n",
    "## Project Overview\n",
    "This project implements a Retrieval-Augmented Generation (RAG) pipeline that answers user questions based on a set of provided documents (PDF/TXT).\n",
    "Instead of relying only on an LLM, the system retrieves the most relevant document chunks and uses them as grounded context to generate accurate responses.\n",
    "\n",
    "**Goal:** Build a clean, modular, and fully functional RAG assistant using embeddings, vector search, and context-based answer generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54579828",
   "metadata": {},
   "source": [
    "## Objective\n",
    "- Ingest 2â€“3 documents (PDF or text files)\n",
    "- Chunk the text into smaller, meaningful sections\n",
    "- Generate vector embeddings using sentence-transformers\n",
    "- Store embeddings in a simple NumPy-based vector search system\n",
    "- Retrieve top-k relevant chunks for any query\n",
    "- Produce grounded answers using an LLM (OpenAI API)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda8c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PyPDF2 import PdfReader\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac22601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb248354",
   "metadata": {},
   "source": [
    "## Document Loading\n",
    "Load PDFs from the folder and extract clean text using PyPDF2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5261e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def load_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        txt = page.extract_text()\n",
    "        if txt:\n",
    "            text += txt + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Path to your single PDF\n",
    "pdf_path = r\"C:\\Users\\shubh\\Documents\\ACE_ACADEMY\\02_AI_Handbook_Assistant_RAG\\python.pdf\"\n",
    "\n",
    "all_text = load_pdf(pdf_path)\n",
    "print(f\"Loaded PDF. Total characters: {len(all_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a74a42",
   "metadata": {},
   "source": [
    "## Text Chunking\n",
    "Split the loaded text into smaller blocks for better retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adfc071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=400, overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(all_text)\n",
    "print(f\"Total chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f603f",
   "metadata": {},
   "source": [
    "## Embedding Generation\n",
    "Generate semantic embeddings for each chunk using SentenceTransformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7eb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "vectors = embed_model.encode(chunks)\n",
    "vectors = np.array(vectors).astype(\"float32\")\n",
    "print(f\"Embeddings ready. Total vectors: {vectors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796888a0",
   "metadata": {},
   "source": [
    "## Retrieval Function\n",
    "Retrieve top-k most relevant chunks for a user query using NumPy cosine similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebe7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, chunks, vectors, k=3):\n",
    "    query_vec = embed_model.encode([query])[0]\n",
    "    a_norm = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    b_norm = query_vec / np.linalg.norm(query_vec)\n",
    "    sims = np.dot(a_norm, b_norm)\n",
    "    top_idx = np.argsort(sims)[-k:][::-1]\n",
    "    return [chunks[i] for i in top_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e34eb4",
   "metadata": {},
   "source": [
    "## RAG Answer Function\n",
    "Combine retrieved chunks with the query and use the OpenAI LLM to generate a grounded answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f08112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(query, chunks, vectors, k=3):\n",
    "    retrieved = retrieve(query, chunks, vectors, k)\n",
    "    context = \"\\n\\n---\\n\".join(retrieved)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant. Answer the user query using ONLY the information below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Final Answer:\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer, retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec28dd40",
   "metadata": {},
   "source": [
    "## Test Queries\n",
    "Check the RAG pipeline by running a few example queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b70489",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What is a variable in Python?\",\n",
    "    \"Explain git commit.\",\n",
    "    \"How do functions work in Python?\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\n=== Query: {q} ===\")\n",
    "    answer, refs = rag_answer(q, chunks, vectors)\n",
    "    print(\"\\nAnswer:\\n\", answer)\n",
    "    print(\"\\nReferences used:\\n\", refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd407c56",
   "metadata": {},
   "source": [
    "## Notes for Submission\n",
    "\n",
    "### âœ… What Worked Well\n",
    "- Text chunking and vector search performed reliably, providing accurate context retrieval.  \n",
    "- SentenceTransformer embeddings captured semantic meaning effectively, making answers grounded and relevant.  \n",
    "- The modular pipeline (loading â†’ chunking â†’ embeddings â†’ retrieval â†’ generation) was easy to follow and extend.\n",
    "\n",
    "### âš ï¸ Challenges Faced\n",
    "- PDF extraction occasionally produced messy or misaligned text, requiring minor cleaning.  \n",
    "- Embedding generation slowed down with very large PDFs.  \n",
    "- Chunk size and overlap had to be tuned for optimal retrieval accuracy.  \n",
    "- Setting up the OpenAI API key on Windows required careful attention.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
